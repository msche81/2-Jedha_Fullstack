{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39378817-9fad-4272-963e-376590f0ae37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+------+\n|  a|  b|  c|   d|     e|\n+---+---+---+----+------+\n|  1|  2|  3|null| apple|\n|  2|  3|  4| 0.0|banana|\n|  3|  4|  5| 1.0|orange|\n+---+---+---+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Démarrer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PandasToSpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Créer un dictionnaire de données\n",
    "data_dict = {'a': [1, 2, 3], 'b': [2, 3, 4], 'c': [3, 4, 5], 'd': [np.nan, 0, 1], 'e': [\"apple\", \"banana\", \"orange\"]}\n",
    "\n",
    "# Créer un DataFrame Pandas\n",
    "pandas_df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "# Convertir le DataFrame Pandas en DataFrame PySpark\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Afficher le DataFrame PySpark\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a9a85a-d4de-4e2a-b02c-5a5b9c235be7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REDSHIFT_USER = 'awsuser'\n",
    "REDSHIFT_PASSWORD = 'Tomatokitten730!?!'\n",
    "\n",
    "REDSHIFT_FULL_PATH = \"jdbc:postgresql://redshift-cluster-1.cmnvi59hnhq5.eu-west-3.redshift.amazonaws.com:5439/dev\"\n",
    "# don't forget to replace \"redshift\" by \"postgresql\"\n",
    "# for example it'll look like:\n",
    "# \"jdbc:postgresql://redshift-cluster-1.csssws1edn9m.eu-west-3.redshift.amazonaws.com:5439/dev\"\n",
    "REDSHIFT_TABLE = 'table_fruits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4db442-d449-4116-80c5-4c8aeabdb958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "\n",
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "df.write.jdbc(url=REDSHIFT_FULL_PATH, table=REDSHIFT_TABLE, mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59aa7d14-8092-4fcb-b6fa-bb93c89de807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+------+\n|  a|  b|  c|   d|     e|\n+---+---+---+----+------+\n|  3|  4|  5| 1.0|orange|\n|  1|  2|  3|null| apple|\n|  2|  3|  4| 0.0|banana|\n+---+---+---+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "table = sqlContext.read.jdbc(url=REDSHIFT_FULL_PATH, table=REDSHIFT_TABLE, properties=properties)\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf56409d-f60b-4a10-8b16-ffb053544e99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3-1-Using Redshift in PySpark-Data-Warehousing.ipynb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
