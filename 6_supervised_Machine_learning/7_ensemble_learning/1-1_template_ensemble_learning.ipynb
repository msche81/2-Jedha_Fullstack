{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# import ensemble methods\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import base estimators\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=DeprecationWarning\n",
    ")  # to avoid deprecation warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = pd.read_csv(\"src/Data.csv\")\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 10\n",
      "Number of columns: 4\n",
      "\n",
      "   Country   Age  Salary Purchased\n",
      "0   France  44.0   72000        No\n",
      "1    Spain  27.0   48000       Yes\n",
      "2  Germany  30.0   54000        No\n",
      "3    Spain  38.0   61000        No\n",
      "4  Germany  40.0   69000       Yes\n",
      "\n",
      "       Country        Age        Salary Purchased\n",
      "count       10   9.000000     10.000000        10\n",
      "unique       3        NaN           NaN         2\n",
      "top     France        NaN           NaN        No\n",
      "freq         4        NaN           NaN         5\n",
      "mean       NaN  38.777778  64300.000000       NaN\n",
      "std        NaN   7.693793  11681.419244       NaN\n",
      "min        NaN  27.000000  48000.000000       NaN\n",
      "25%        NaN  35.000000  55000.000000       NaN\n",
      "50%        NaN  38.000000  64000.000000       NaN\n",
      "75%        NaN  44.000000  71250.000000       NaN\n",
      "max        NaN  50.000000  83000.000000       NaN\n",
      "\n",
      "Country       0.0\n",
      "Age          10.0\n",
      "Salary        0.0\n",
      "Purchased     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics and dataset overview\n",
    "print(f\"Number of rows: {dataset.shape[0]}\")\n",
    "print(f\"Number of columns: {dataset.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(dataset.head())\n",
    "print()\n",
    "\n",
    "# Basic statistics\n",
    "data_desc = dataset.describe(include=\"all\")\n",
    "print(data_desc)\n",
    "print()\n",
    "\n",
    "# Percentage of missing values for each column\n",
    "missing_values = (dataset.isnull().sum() / dataset.shape[0]) * 100\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n",
      "Y (Target variable):\n",
      "0     No\n",
      "1    Yes\n",
      "2     No\n",
      "3     No\n",
      "4    Yes\n",
      "Name: Purchased, dtype: object\n",
      "\n",
      "X (Features):\n",
      "   Country   Age  Salary\n",
      "0   France  44.0   72000\n",
      "1    Spain  27.0   48000\n",
      "2  Germany  30.0   54000\n",
      "3    Spain  38.0   61000\n",
      "4  Germany  40.0   69000\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "target_variable = \"Purchased\"\n",
    "\n",
    "X = dataset.drop(columns=[target_variable], axis= 1)\n",
    "Y = dataset[target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print(\"Y (Target variable):\")\n",
    "print(Y.head())\n",
    "print(\"\\nX (Features):\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_features:\n",
      "['Age', 'Salary']\n",
      "\n",
      "categorical_features:\n",
      "['Country']\n"
     ]
    }
   ],
   "source": [
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns.tolist()\n",
    "print(\"numeric_features:\")\n",
    "print(numeric_features)\n",
    "print(\"\\ncategorical_features:\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop=\"first\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing preprocessings on train set...\n",
      "X_train done\n",
      "[[ 1.61706195e+00  1.46885753e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.22715727e-01  1.09777773e+00  1.00000000e+00  0.00000000e+00]\n",
      " [-1.41104234e-15 -1.00500778e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 2.26956063e-01  8.50391200e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.25542617e-01  1.08231607e-01  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      "Label Encoding on train set:\n",
      "y_train done\n",
      "[0 1 0 1 0]\n",
      "\n",
      "Performing preprocessings on test set...\n",
      "[[2.80858127 2.82948345 1.         0.        ]\n",
      " [2.41140816 2.33471038 0.         0.        ]]\n",
      "\n",
      "Encoding labels...\n",
      "8     No\n",
      "7    Yes\n",
      "Name: Purchased, dtype: object\n",
      "...Done\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on train set\n",
    "print(\"Performing preprocessings on train set...\")\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "print(\"X_train done\")\n",
    "print(X_train[0:5])\n",
    "print()\n",
    "\n",
    "# Label encoding\n",
    "print(\"Label Encoding on train set:\")\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "print(\"y_train done\")\n",
    "print(y_train[0:5])\n",
    "print()\n",
    "\n",
    "# Preprocessings on test set\n",
    "print(\"Performing preprocessings on test set...\")\n",
    "X_test = preprocessor.transform(X_test)\n",
    "print(X_test[0:5,:])\n",
    "print()\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(y_test[0:5])\n",
    "Y_test = encoder.transform(y_test)\n",
    "print(\"...Done\")\n",
    "print(Y_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging with logistic regression as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END ..................estimator__C=0.01, n_estimators=5; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.01, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=10; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.01, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=10; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.05, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=10; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.05, n_estimators=5; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.05, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=20; total time=   0.1s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=30; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=10; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=20; total time=   0.1s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=10; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=20; total time=   0.1s\n",
      "[CV] END ...................estimator__C=0.1, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=10; total time=   0.0s\n",
      "[CV] END ...................estimator__C=0.1, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=30; total time=   0.1s\n",
      "[CV] END ...................estimator__C=0.1, n_estimators=5; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.01, n_estimators=30; total time=   0.1s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=10; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=10; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=20; total time=   0.1s\n",
      "[CV] END ...................estimator__C=0.5, n_estimators=5; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=20; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=20; total time=   0.0s\n",
      "[CV] END ...................estimator__C=0.5, n_estimators=5; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=20; total time=   0.0s\n",
      "[CV] END ...................estimator__C=0.5, n_estimators=5; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=10; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=30; total time=   0.1s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=10; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=10; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=20; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=30; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=30; total time=   0.1s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=10; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=20; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=20; total time=   0.1s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=20; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=30; total time=   0.1s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=30; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=20; total time=   0.0s\n",
      "[CV] END .................estimator__C=0.05, n_estimators=30; total time=   0.1s\n",
      "[CV] END ..................estimator__C=0.1, n_estimators=30; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=30; total time=   0.0s\n",
      "[CV] END ..................estimator__C=0.5, n_estimators=30; total time=   0.0s\n",
      "Grid search completed.\n",
      "Best hyperparameters: {'estimator__C': 0.5, 'n_estimators': 10}\n",
      "Best cross-validation accuracy: 0.5000\n",
      "Training accuracy: 0.7500\n",
      "Test accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Initialize logistic regression with increased max_iter to handle convergence warnings\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize BaggingClassifier with logistic regression as the base estimator\n",
    "model = BaggingClassifier(estimator=logistic_regression)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "params = {\n",
    "    \"estimator__C\": [0.01, 0.05, 0.1, 0.5],  # C is a hyperparameter of LogisticRegression\n",
    "    \"n_estimators\": [5, 10, 20, 30]  # n_estimators is a hyperparameter of BaggingClassifier\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 3-fold cross-validation\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    n_jobs=-1,  # Utilize all available cores for parallel processing\n",
    "    verbose=2   # Increase verbosity for detailed output\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search...\")\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Display results\n",
    "print(f\"Best hyperparameters: {gridsearch.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {gridsearch.best_score_:.4f}\")\n",
    "print(f\"Training accuracy: {gridsearch.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test accuracy: {gridsearch.score(X_test, Y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging with decision tree as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search...\n",
      "Fitting 3 folds for each of 135 candidates, totalling 405 fits\n",
      "Grid search complete.\n",
      "Best hyperparameters: {'estimator__max_depth': 1, 'estimator__min_samples_leaf': 2, 'estimator__min_samples_split': 2, 'n_estimators': 4}\n",
      "Best cross-validation accuracy: 0.6111\n",
      "Training set accuracy: 0.6250\n",
      "Test set accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marie-sophiechenevier/Library/CloudStorage/Dropbox/8-Jedha/GitHub/2-Jedha_Fullstack/.venv/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle de base et de l'algorithme d'ensemble\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize BaggingClassifier with decision_tree as the base estimator\n",
    "model = BaggingClassifier(estimator=decision_tree)\n",
    "\n",
    "# Grille d'hyperparamètres pour la recherche par grille\n",
    "params = {\n",
    "    \"estimator__max_depth\": [1, 2, 3],\n",
    "    \"estimator__min_samples_leaf\": [1, 2, 3],\n",
    "    \"estimator__min_samples_split\": [2, 3, 4],\n",
    "    \"n_estimators\": [2, 4, 6, 8, 10],\n",
    "}\n",
    "\n",
    "# Exécution de la recherche par grille avec validation croisée\n",
    "print(\"Running grid search...\")\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    cv=3, verbose=1,\n",
    "    n_jobs=-1  # Parallel processing for efficiency\n",
    ")\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs paramètres et performances\n",
    "print(\"Grid search complete.\")\n",
    "print(f\"Best hyperparameters: {gridsearch.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {gridsearch.best_score_:.4f}\")\n",
    "\n",
    "# Évaluation des performances sur les ensembles d'entraînement et de test\n",
    "train_accuracy = gridsearch.score(X_train, y_train)\n",
    "test_accuracy = gridsearch.score(X_test, Y_test)\n",
    "print(f\"Training set accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN or infinite values in X_train and X_test...\n",
      "NaN values in X_train: 0\n",
      "NaN values in X_test: 0\n",
      "Infinite values in X_train: 0\n",
      "Infinite values in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# vérifier si X_train et X_test contiennent des valeurs manquantes ou infinies :\n",
    "print(\"Checking for NaN or infinite values in X_train and X_test...\")\n",
    "print(\"NaN values in X_train:\", np.isnan(X_train).sum())\n",
    "print(\"NaN values in X_test:\", np.isnan(X_test).sum())\n",
    "print(\"Infinite values in X_train:\", np.isinf(X_train).sum())\n",
    "print(\"Infinite values in X_test:\", np.isinf(X_test).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train: float64\n",
      "Data type of X_test: float64\n"
     ]
    }
   ],
   "source": [
    "# vérifier le type de données\n",
    "print(\"Data type of X_train:\", X_train.dtype)\n",
    "print(\"Data type of X_test:\", X_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost with logistic regression as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Grid search complete.\n",
      "Best hyperparameters: {'estimator__C': 0.5, 'n_estimators': 5}\n",
      "Best cross-validation accuracy: 0.6111\n",
      "Training set accuracy: 0.8750\n",
      "Test set accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle de base et de l'algorithme d'ensemble\n",
    "logistic_regression = LogisticRegression(max_iter=1000)  # max_iter augmenté pour éviter les avertissements de convergence\n",
    "model = AdaBoostClassifier(estimator=logistic_regression, algorithm='SAMME')  # Utilisation explicite de SAMME pour éviter l'avertissement\n",
    "\n",
    "# Grille d'hyperparamètres pour la recherche par grille\n",
    "params = {\n",
    "    \"estimator__C\": [0.01, 0.05, 0.1, 0.5],  # 'estimator__' nécessaire pour les hyperparamètres de LogisticRegression\n",
    "    \"n_estimators\": [5, 10, 20, 30],         # Hyperparamètre de AdaBoost pour le nombre d'estimateurs\n",
    "}\n",
    "\n",
    "# Exécution de la recherche par grille avec validation croisée\n",
    "print(\"Running grid search...\")\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    verbose=1,  # Affiche la progression de la recherche\n",
    "    n_jobs=-1   # Utilise tous les cœurs de processeur pour accélérer la recherche\n",
    ")\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs paramètres et performances\n",
    "print(\"Grid search complete.\")\n",
    "print(f\"Best hyperparameters: {gridsearch.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {gridsearch.best_score_:.4f}\")\n",
    "\n",
    "# Évaluation des performances sur les ensembles d'entraînement et de test\n",
    "train_accuracy = gridsearch.score(X_train, y_train)\n",
    "test_accuracy = gridsearch.score(X_test, Y_test)\n",
    "print(f\"Training set accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost with decision tree as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search...\n",
      "Fitting 3 folds for each of 135 candidates, totalling 405 fits\n",
      "Grid search complete.\n",
      "Best hyperparameters: {'estimator__max_depth': 1, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'n_estimators': 2}\n",
      "Best cross-validation accuracy: 0.3889\n",
      "Training set accuracy: 0.6250\n",
      "Test set accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle de base et de l'algorithme d'ensemble avec SAMME pour éviter l'avertissement de dépréciation\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "model = AdaBoostClassifier(estimator=decision_tree, algorithm='SAMME')\n",
    "\n",
    "# Grille d'hyperparamètres pour la recherche par grille\n",
    "params = {\n",
    "    \"estimator__max_depth\": [1, 2, 3],           # Profondeur de l'arbre de décision\n",
    "    \"estimator__min_samples_leaf\": [1, 2, 3],    # Nombre min. d'échantillons par feuille\n",
    "    \"estimator__min_samples_split\": [2, 3, 4],   # Nombre min. d'échantillons pour diviser un nœud\n",
    "    \"n_estimators\": [2, 4, 6, 8, 10],            # Nombre d'estimateurs dans AdaBoost\n",
    "}\n",
    "\n",
    "# Exécution de la recherche par grille avec validation croisée\n",
    "print(\"Running grid search...\")\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    verbose=1,  # Affiche la progression de la recherche\n",
    "    n_jobs=-1   # Utilise tous les cœurs pour accélérer la recherche\n",
    ")\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs paramètres et performances\n",
    "print(\"Grid search complete.\")\n",
    "print(f\"Best hyperparameters: {gridsearch.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {gridsearch.best_score_:.4f}\")\n",
    "\n",
    "# Évaluation des performances sur les ensembles d'entraînement et de test\n",
    "train_accuracy = gridsearch.score(X_train, y_train)\n",
    "test_accuracy = gridsearch.score(X_test, Y_test)\n",
    "print(f\"Training set accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn's GradientBoosting\n",
    "Boosting with logistic regression as base estimator: forbidden ⛔️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
