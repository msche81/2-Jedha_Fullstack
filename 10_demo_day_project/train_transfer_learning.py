import os
import mlflow
import mlflow.tensorflow
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from data_pipeline import get_data  # Chargement des datasets

# ğŸ“ˆ ParamÃ¨tres gÃ©nÃ©raux
batch_size = 32
img_size = (224, 224)
num_classes = 3  # Dry, Normal, Oily
learning_rate = 0.0005  # Plus petit pour Ã©viter de casser les poids prÃ©-entraÃ®nÃ©s
num_epochs = 30  # Moins d'epochs car le modÃ¨le est dÃ©jÃ  partiellement entraÃ®nÃ©

# ğŸ‘‰ Initialisation de MLflow
mlflow.set_experiment("transfer_learning_skin_type")

# ğŸ‘‰ Chargement des donnÃ©es
train_generator, valid_generator, test_generator, _ = get_data(batch_size, img_size)

with mlflow.start_run():
    # ğŸ‘‰ Enregistrement des hyperparamÃ¨tres
    mlflow.log_param("batch_size", batch_size)
    mlflow.log_param("img_size", img_size)
    mlflow.log_param("learning_rate", learning_rate)
    mlflow.log_param("num_epochs", num_epochs)

    # ğŸ‘‰ Charger MobileNetV2 sans la couche de classification finale
    base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
    base_model.trainable = False  # âŒ On ne fine-tune pas les couches prÃ©-entraÃ®nÃ©es (pour l'instant)

    # ğŸ‘‰ Construction du modÃ¨le
    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        BatchNormalization(),
        Dense(128, activation="relu"),
        Dropout(0.5),
        Dense(64, activation="relu"),
        Dropout(0.5),
        Dense(num_classes, activation="softmax")
    ])

    # ğŸ‘‰ Compilation du modÃ¨le
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )

    # ğŸ‘‰ Enregistrement du modÃ¨le dans MLflow avant entraÃ®nement
    mlflow.tensorflow.autolog()

    # ğŸ‘‰ EntraÃ®nement du modÃ¨le
    history = model.fit(
        train_generator,
        epochs=num_epochs,
        validation_data=valid_generator,
	    verbose=1  # ğŸ” Afficher les logs Ã  chaque epoch
    )

    # ğŸ‘‰ Enregistrement des mÃ©triques finales
    final_loss, final_acc = model.evaluate(valid_generator)
    mlflow.log_metric("final_loss", final_loss)
    mlflow.log_metric("final_accuracy", final_acc)

    # ğŸ‘‰ Enregistrement du modÃ¨le dans MLflow
    mlflow.tensorflow.log_model(model, "transfer_learning_model")
    
    print(f"âœ… ModÃ¨le MobileNetV2 enregistrÃ© dans MLflow avec une accuracy de {final_acc:.4f}")

# ğŸ‘‰ Affichage du rÃ©sumÃ© du modÃ¨le
model.summary()

