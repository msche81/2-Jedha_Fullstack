{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/A0qay0vvejOauGnbyRY7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cBI24QJTuxka","executionInfo":{"status":"ok","timestamp":1736353485624,"user_tz":-60,"elapsed":8795,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"outputs":[],"source":["# Import Tensorflow & Pathlib librairies\n","import tensorflow as tf\n","import pathlib\n","import pandas as pd\n","import os\n","import io\n","import warnings\n","warnings.filterwarnings('ignore')\n","import json\n","from random import randint\n","from numpy import array\n","from numpy import argmax\n","from numpy import array_equal\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"code","source":["input_dim = 100\n","input_seq_len = 10\n","target_seq_len = 5"],"metadata":{"id":"K7vNJX9mu1qB","executionInfo":{"status":"ok","timestamp":1736353511844,"user_tz":-60,"elapsed":227,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# generate a sequence of random integers from 2 to n_unique\n","def generate_sequence(length, n_unique):\n","\treturn [randint(2, n_unique-1) for _ in range(length)]"],"metadata":{"id":"JkSiKpgWu_do","executionInfo":{"status":"ok","timestamp":1736353535170,"user_tz":-60,"elapsed":257,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["generate_sequence(input_seq_len,input_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwWrm3fdvFJl","executionInfo":{"status":"ok","timestamp":1736353547649,"user_tz":-60,"elapsed":232,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"f2a2a26d-5fe9-4888-c832-42e086e7ad40"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[69, 87, 33, 81, 57, 19, 21, 17, 81, 41]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# prepare data\n","def get_dataset(n_in, n_out, cardinality, n_samples, printing=False):\n","  X1, y = list(), list()\n","  for _ in range(n_samples):\n","    # generate source sequence\n","    source = generate_sequence(n_in, cardinality)\n","    source_pad = source\n","    if printing:\n","      print(\"source:\", source_pad)\n","    # define padded target sequence\n","    # we add the <start> token at the beginning of each sequence\n","    # here we'll simply consider that the start token will coded\n","    # by the index 0\n","    target = source[:n_out]\n","    target.reverse()\n","    target = [0] + target\n","    if printing:\n","      print(\"target:\", target)\n","    # store\n","    X1.append(source_pad)\n","    y.append(target)\n","  return array(X1), array(y)\n"],"metadata":{"id":"WKbx4SRovIMy","executionInfo":{"status":"ok","timestamp":1736353619967,"user_tz":-60,"elapsed":330,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["input, target =  get_dataset(input_seq_len,target_seq_len,input_dim,1,True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKnPrlsIvPn6","executionInfo":{"status":"ok","timestamp":1736355063052,"user_tz":-60,"elapsed":266,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"f0afbd8c-2ea6-4d26-a439-7b2a8f49730b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["source: [56, 58, 72, 73, 72, 81, 91, 64, 78, 80]\n","target: [0, 72, 73, 72, 58, 56]\n"]}]},{"cell_type":"code","source":["X_train, y_train = get_dataset(input_seq_len,target_seq_len,input_dim,10000)\n","X_val, y_val = get_dataset(input_seq_len,target_seq_len,input_dim,5000)"],"metadata":{"id":"P1-fmIx506KT","executionInfo":{"status":"ok","timestamp":1736355169886,"user_tz":-60,"elapsed":476,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 128\n","train_batch = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)"],"metadata":{"id":"utYoYsHr1UMp","executionInfo":{"status":"ok","timestamp":1736355221342,"user_tz":-60,"elapsed":225,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# let's start by defining the number of units needed for the embedding and\n","# the lstm layers\n","\n","n_embed = 32\n","n_gru = 32"],"metadata":{"id":"3g6yrayS1g0X","executionInfo":{"status":"ok","timestamp":1736355324908,"user_tz":-60,"elapsed":221,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class encoder_maker(tf.keras.Model):\n","  def __init__(self, in_vocab_size, embed_dim, n_units):\n","    super().__init__()\n","    # instanciate an embedding layer\n","    self.n_units = n_units\n","    self.embed = tf.keras.layers.Embedding(input_dim=in_vocab_size,\n","                                      output_dim=embed_dim)\n","    # instantiate GRU layer\n","    self.gru = tf.keras.layers.GRU(units=n_units,\n","                              return_sequences=True,\n","                              return_state=True)\n","  def __call__(self, input_batch):\n","    # each output will be saved as a class attribute so we can easily access\n","    # them to control the shapes throughout the demo\n","    self.embed_out = self.embed(input_batch)\n","    self.gru_out, self.gru_state = self.gru(self.embed_out)#, initial_state=initial_state)\n","\n","    return self.gru_out, self.gru_state"],"metadata":{"id":"3oexdhpH16Gw","executionInfo":{"status":"ok","timestamp":1736355351571,"user_tz":-60,"elapsed":241,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["encoder = encoder_maker(input_dim, n_embed, n_gru)"],"metadata":{"id":"-nhHwL1A2Am8","executionInfo":{"status":"ok","timestamp":1736355464859,"user_tz":-60,"elapsed":296,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["encoder_output, encoder_state = encoder(tf.expand_dims(X_train[0],0))"],"metadata":{"id":"fFn5wtPH2cP-","executionInfo":{"status":"ok","timestamp":1736355527565,"user_tz":-60,"elapsed":592,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["encoder_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PQxXq5b2rfC","executionInfo":{"status":"ok","timestamp":1736355561270,"user_tz":-60,"elapsed":232,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"7594d379-3841-4526-c249-2c97faa96d94"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 10, 32), dtype=float32, numpy=\n","array([[[ 3.03471158e-03,  1.91234946e-02, -1.36195887e-02,\n","          2.48559099e-03,  2.30035726e-02,  2.49295104e-02,\n","          1.63328983e-02,  1.14342822e-02,  9.42002807e-04,\n","          9.88711137e-03, -1.04962080e-03, -2.26437091e-03,\n","         -1.97378155e-02, -1.05690500e-02, -2.70593562e-03,\n","          1.54119385e-02, -8.41972884e-03, -4.31373250e-03,\n","         -6.67968392e-03,  6.39934372e-03,  7.94641115e-03,\n","         -1.73442066e-02,  1.59114115e-02,  4.08182526e-03,\n","          6.79645035e-03,  4.44710162e-03, -6.88423403e-03,\n","         -3.67930345e-03, -2.22801547e-02,  3.42548359e-03,\n","         -8.58697458e-04, -3.19993962e-03],\n","        [ 2.16051973e-02,  2.24431464e-03, -1.57597549e-02,\n","          9.76931863e-03,  2.22305581e-03,  1.70954987e-02,\n","          2.40932256e-02, -7.54060410e-03,  1.75215621e-02,\n","          1.70840099e-02, -1.69410987e-03, -1.04280300e-02,\n","         -1.74684562e-02, -2.23493669e-04, -3.54750897e-03,\n","          6.14586659e-03,  9.41119250e-03, -6.83985185e-03,\n","          2.42277794e-03, -4.23475774e-03,  1.25452876e-04,\n","         -4.67974693e-04,  9.06614680e-03, -1.25553980e-02,\n","          1.63408846e-03, -1.90083170e-03, -1.08379684e-03,\n","          3.47145740e-03, -1.02167763e-03, -6.27491809e-03,\n","          2.89590959e-03, -1.70720927e-02],\n","        [-1.24653829e-02,  2.59663165e-03, -5.29047940e-03,\n","         -1.84880123e-02,  1.89610780e-03, -6.92474283e-03,\n","          2.76009236e-02, -1.66748953e-03,  3.00128758e-03,\n","          1.03975087e-02,  9.17387567e-03,  1.67400669e-03,\n","         -4.69689444e-03, -6.52225409e-03, -1.06092282e-02,\n","         -4.03585378e-03,  3.07440292e-04, -4.60922485e-03,\n","         -1.13443378e-02,  8.27262271e-03,  5.09404344e-03,\n","          6.48130022e-04,  5.17906528e-03, -5.79693867e-03,\n","         -1.49813965e-02, -1.81161426e-02,  1.86626539e-02,\n","          4.05359315e-03, -9.15501034e-04,  6.79842569e-03,\n","          2.02124845e-03, -1.11458320e-02],\n","        [-1.55314151e-02, -1.08846358e-03,  6.93994109e-03,\n","         -2.01312248e-02,  1.08609376e-02,  7.41329044e-04,\n","          5.25997300e-03,  1.46765001e-02,  2.26475578e-03,\n","         -4.41188132e-03, -2.51064962e-03,  6.66468451e-03,\n","          4.70475107e-03,  1.85990427e-02, -1.37435952e-02,\n","         -2.85483664e-04,  2.42493879e-02, -4.30518761e-04,\n","         -6.68963464e-03, -1.61028095e-03,  7.25868158e-03,\n","          2.85877637e-03,  1.22057199e-02, -1.52879581e-02,\n","         -1.62738934e-02, -1.30596794e-02,  4.65232320e-03,\n","         -1.51153468e-02,  2.53568147e-03,  8.79084878e-03,\n","          5.76287508e-03, -1.28051871e-02],\n","        [-7.99575727e-03, -6.17461186e-03,  2.05279444e-03,\n","          1.60204601e-02, -1.68544659e-03,  1.38208335e-02,\n","         -3.56990821e-03,  1.23784831e-03,  3.55909159e-03,\n","          4.72019613e-03, -9.33492370e-03, -5.72660938e-04,\n","          8.22252966e-03,  1.50974253e-02, -1.97888762e-02,\n","          7.75763066e-03, -6.85155671e-03,  2.93004606e-03,\n","         -9.72607639e-04, -9.35412711e-04,  4.05127043e-03,\n","          4.44713607e-03,  4.90473071e-03, -5.42080961e-03,\n","         -9.85007361e-03, -7.05686072e-03,  8.46222695e-03,\n","         -1.02586281e-02, -5.62261790e-03,  1.12823462e-02,\n","          1.30403191e-02, -2.28633005e-02],\n","        [-4.27824073e-03, -9.92015935e-03, -1.15567481e-03,\n","          3.42653096e-02, -8.45632609e-03,  2.31619608e-02,\n","         -6.92986464e-03, -6.26033079e-03,  3.60480929e-03,\n","          9.48115718e-03, -1.38402283e-02, -2.28908751e-03,\n","          1.11536309e-02,  1.37131624e-02, -2.29768045e-02,\n","          1.33171985e-02, -2.25283541e-02,  3.14638764e-03,\n","          4.49696742e-03,  1.54490245e-03,  2.79738149e-03,\n","          4.26397100e-03,  5.20046218e-04, -1.78762490e-03,\n","         -2.66398629e-03, -5.47129009e-03,  8.68932158e-03,\n","         -5.71051193e-03, -1.01294611e-02,  1.38912285e-02,\n","          1.64387692e-02, -2.61416938e-02],\n","        [ 1.22601986e-02, -6.48233481e-03, -1.59638226e-02,\n","          2.04527415e-02, -2.07487121e-02,  7.69079104e-03,\n","          9.88533534e-03, -1.03543904e-02, -3.29083996e-03,\n","          1.33278649e-02, -4.15227609e-03, -7.62351323e-03,\n","          8.57208483e-03, -7.61813018e-04, -1.94276739e-02,\n","          7.08340714e-03, -3.68307009e-02, -1.02535449e-02,\n","          8.92148353e-03,  4.51924838e-03, -6.39448641e-04,\n","         -6.83369581e-03, -1.73279382e-02, -1.08315488e-02,\n","          1.05045801e-02, -1.21878870e-02,  1.29867792e-02,\n","          4.14679060e-04,  1.84047967e-03,  2.87258383e-02,\n","          5.63616119e-03, -8.84274952e-03],\n","        [-1.54378507e-02,  5.86462859e-03, -2.28682235e-02,\n","         -3.91427986e-03, -9.33426898e-03, -2.68363813e-03,\n","          8.99309758e-03, -8.26856773e-03, -1.46083934e-02,\n","          1.26843126e-02, -1.14979204e-02, -6.46616379e-03,\n","          5.57277165e-03, -1.40630025e-02, -2.41057463e-02,\n","          2.49329023e-02, -2.30047088e-02, -1.51447225e-02,\n","         -1.53028546e-03,  1.23850291e-03, -7.90180732e-03,\n","         -9.39423684e-03, -4.63894382e-03,  5.58434427e-03,\n","          1.08463317e-02, -1.07574435e-02,  1.86488666e-02,\n","         -1.88538712e-02, -1.46836899e-02,  4.17101309e-02,\n","         -5.89532824e-03,  3.82559700e-03],\n","        [ 7.52507057e-03,  1.59059297e-02, -1.25476792e-02,\n","          8.09223391e-03, -1.45941041e-02, -4.64849593e-03,\n","         -1.31875230e-02, -1.41606536e-02, -9.46972705e-03,\n","          2.30598282e-02, -1.35769267e-02,  9.13003739e-03,\n","         -7.39519484e-04, -1.74837187e-03, -2.50677317e-02,\n","          2.29444876e-02, -3.95971835e-02, -1.32361706e-02,\n","          7.02819368e-03, -6.99482299e-03,  5.82186924e-03,\n","         -6.64865598e-03,  1.69683471e-02, -7.14909192e-03,\n","          1.11757256e-02, -3.13854241e-03,  2.21305434e-02,\n","         -7.78870750e-03, -4.87428484e-03,  1.16579570e-02,\n","         -1.17746508e-02,  7.73201184e-03],\n","        [-5.92978811e-03,  1.51975434e-02, -1.17299147e-02,\n","         -1.12715829e-02, -1.23786125e-02, -1.12885181e-02,\n","         -1.97602231e-02, -1.56078609e-02, -7.50952400e-03,\n","          1.82237048e-02, -1.18089588e-02,  2.07810234e-02,\n","         -4.86983359e-03,  9.44465981e-04, -4.85745212e-03,\n","          3.37691456e-02, -2.78534610e-02, -2.16224883e-02,\n","          1.40965628e-02,  6.25297986e-03,  5.49449120e-03,\n","          2.12026667e-03,  2.40481235e-02,  5.28925937e-03,\n","         -1.35594653e-03,  2.99488893e-05,  2.91493759e-02,\n","          2.46301014e-03, -7.44984811e-03,  1.41734518e-02,\n","         -1.22115742e-02,  1.14360941e-03]]], dtype=float32)>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["encoder_state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgvg5Kxw2zTU","executionInfo":{"status":"ok","timestamp":1736355592732,"user_tz":-60,"elapsed":210,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"6b19435b-9e58-4361-d882-f1520e6a3e0e"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n","array([[-5.92978811e-03,  1.51975434e-02, -1.17299147e-02,\n","        -1.12715829e-02, -1.23786125e-02, -1.12885181e-02,\n","        -1.97602231e-02, -1.56078609e-02, -7.50952400e-03,\n","         1.82237048e-02, -1.18089588e-02,  2.07810234e-02,\n","        -4.86983359e-03,  9.44465981e-04, -4.85745212e-03,\n","         3.37691456e-02, -2.78534610e-02, -2.16224883e-02,\n","         1.40965628e-02,  6.25297986e-03,  5.49449120e-03,\n","         2.12026667e-03,  2.40481235e-02,  5.28925937e-03,\n","        -1.35594653e-03,  2.99488893e-05,  2.91493759e-02,\n","         2.46301014e-03, -7.44984811e-03,  1.41734518e-02,\n","        -1.22115742e-02,  1.14360941e-03]], dtype=float32)>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, attention_units):\n","        super().__init__()\n","        # Dense layers pour calculer les scores\n","        self.W1 = tf.keras.layers.Dense(units=attention_units)  # Pour enc_out\n","        self.W2 = tf.keras.layers.Dense(units=attention_units)  # Pour state\n","        self.V = tf.keras.layers.Dense(units=1)  # Pour générer un score scalaire\n","\n","    def call(self, enc_out, state):\n","        \"\"\"\n","        Args:\n","        - enc_out: Sorties de l'encodeur (batch_size, seq_len, enc_units)\n","        - state: Dernier état caché (batch_size, enc_units)\n","\n","        Returns:\n","        - context_vector: Résumé pondéré des sorties de l'encodeur (batch_size, enc_units)\n","        - attention_weights: Poids d'attention (batch_size, seq_len, 1)\n","        \"\"\"\n","\n","        # 1. Appliquer W1 sur les sorties de l'encodeur\n","        W1_out = self.W1(enc_out)  # (batch_size, seq_len, attention_units)\n","\n","        # 2. Appliquer W2 sur l'état caché\n","        state = tf.expand_dims(state, axis=1)  # Ajouter une dimension : (batch_size, 1, enc_units)\n","        W2_out = self.W2(state)  # (batch_size, 1, attention_units)\n","\n","        # 3. Somme des résultats de W1 et W2\n","        sum_scores = tf.nn.tanh(W1_out + W2_out)  # (batch_size, seq_len, attention_units)\n","\n","        # 4. Calculer le score attentionnel avec V\n","        scores = self.V(sum_scores)  # (batch_size, seq_len, 1)\n","\n","        # 5. Calculer les poids d'attention avec softmax\n","        attention_weights = tf.nn.softmax(scores, axis=1)  # (batch_size, seq_len, 1)\n","\n","        # 6. Calculer le vecteur de contexte (somme pondérée des sorties encodeur)\n","        context_vector = tf.reduce_sum(enc_out * attention_weights, axis=1)  # (batch_size, enc_units)\n","\n","        return context_vector, attention_weights"],"metadata":{"id":"wf4QyfNi27ft","executionInfo":{"status":"ok","timestamp":1736357663419,"user_tz":-60,"elapsed":241,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["attention_layer = BahdanauAttention(8)"],"metadata":{"id":"k3jwVl-h3Q7s","executionInfo":{"status":"ok","timestamp":1736357714134,"user_tz":-60,"elapsed":251,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, tar_vocab_size, embed_dim, n_units):\n","        super().__init__()\n","\n","        # Couche d'embedding pour encoder les séquences cibles\n","        self.embedding = tf.keras.layers.Embedding(input_dim=tar_vocab_size, output_dim=embed_dim)\n","\n","        # Couche GRU pour gérer la séquence de décodage\n","        self.gru = tf.keras.layers.GRU(units=n_units, return_sequences=True, return_state=True)\n","\n","        # Couche Dense pour générer une distribution sur le vocabulaire cible\n","        self.dense = tf.keras.layers.Dense(units=tar_vocab_size, activation=\"softmax\")\n","\n","        # Couche d'attention (Bahdanau)\n","        self.attention = BahdanauAttention(attention_units=n_units)\n","\n","    def call(self, dec_in, enc_out, state):\n","        \"\"\"\n","        Args:\n","        - dec_in: Entrée du décodeur (batch_size, 1)\n","        - enc_out: Sorties de l'encodeur (batch_size, seq_len, enc_units)\n","        - state: Dernier état caché du décodeur (batch_size, n_units)\n","\n","        Returns:\n","        - pred_out: Prédictions (batch_size, tar_vocab_size)\n","        - state: Nouvel état caché (batch_size, n_units)\n","        - attention_weights: Poids d'attention (batch_size, seq_len, 1)\n","        \"\"\"\n","\n","        # 1. Appliquer la couche d'attention\n","        context_vector, attention_weights = self.attention(enc_out, state)\n","\n","        # 2. Encoder l'entrée cible avec l'embedding\n","        embedded_in = self.embedding(dec_in)  # (batch_size, 1, embed_dim)\n","\n","        # 3. Ajouter une dimension au vecteur de contexte et concaténer\n","        context_vector = tf.expand_dims(context_vector, axis=1)  # (batch_size, 1, n_units)\n","        concat_input = tf.concat([embedded_in, context_vector], axis=-1)  # (batch_size, 1, embed_dim + n_units)\n","\n","        # 4. Passer dans la couche GRU\n","        gru_out, state = self.gru(concat_input)  # (batch_size, 1, n_units), (batch_size, n_units)\n","\n","        # 5. Appliquer la couche Dense pour générer les prédictions\n","        pred_out = self.dense(tf.reshape(gru_out, shape=(-1, gru_out.shape[2])))  # (batch_size, tar_vocab_size)\n","\n","        return pred_out, state, attention_weights"],"metadata":{"id":"gs92Cega3j5z","executionInfo":{"status":"ok","timestamp":1736357740186,"user_tz":-60,"elapsed":226,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(tar_vocab_size=input_dim, embed_dim=n_embed, n_units=n_gru)"],"metadata":{"id":"6QchgD1E4J7K","executionInfo":{"status":"ok","timestamp":1736357792470,"user_tz":-60,"elapsed":218,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["decoder_input = tf.expand_dims(tf.expand_dims(y_train[0][0], axis=0), axis=0) # the teacher forcing is\n","# the first element of the target sequence which corresponds to the <start> token\n","# we use expand dim to artificially add the batch size dimension"],"metadata":{"id":"L2FRZqbx4WNm","executionInfo":{"status":"ok","timestamp":1736357802944,"user_tz":-60,"elapsed":225,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["decoder_input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3-TT5bUs4RXe","executionInfo":{"status":"ok","timestamp":1736357835684,"user_tz":-60,"elapsed":367,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"11a00ae4-b1af-4ed7-a4d0-ad60a2671ff8"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]])>"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["decoder(decoder_input,encoder_output,encoder_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7OssykXk4SwH","executionInfo":{"status":"ok","timestamp":1736357838646,"user_tz":-60,"elapsed":565,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"51d8ccbd-9990-44fd-d9a0-b81dc23d6fa9"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n"," array([[0.00997438, 0.00989334, 0.00993614, 0.00995874, 0.01003727,\n","         0.01003644, 0.0100796 , 0.00992493, 0.0100101 , 0.01004987,\n","         0.01006421, 0.01005374, 0.01000514, 0.01003642, 0.01007826,\n","         0.01002845, 0.01001302, 0.01007884, 0.01005108, 0.0099602 ,\n","         0.01001415, 0.0099773 , 0.01007107, 0.01006327, 0.00994133,\n","         0.01007286, 0.00997732, 0.01004551, 0.01000611, 0.01001026,\n","         0.01001573, 0.00999367, 0.01001278, 0.01006018, 0.00993674,\n","         0.00996929, 0.00993958, 0.01001803, 0.01005396, 0.00995046,\n","         0.01000345, 0.00997876, 0.00997254, 0.01002832, 0.00996728,\n","         0.01005219, 0.00999688, 0.00996984, 0.01009161, 0.0100107 ,\n","         0.01003092, 0.00998605, 0.0100986 , 0.00999198, 0.01003785,\n","         0.01004173, 0.01000814, 0.0100536 , 0.00998374, 0.01000855,\n","         0.01006149, 0.00997662, 0.00996176, 0.00992897, 0.01006742,\n","         0.00994661, 0.00995713, 0.00997395, 0.0099872 , 0.01006193,\n","         0.00997097, 0.00993399, 0.00996875, 0.01002056, 0.00995903,\n","         0.01001347, 0.00989798, 0.00989337, 0.00998138, 0.00996949,\n","         0.00999897, 0.01005291, 0.01009246, 0.0099142 , 0.00990721,\n","         0.01011016, 0.00996031, 0.00992432, 0.00995573, 0.01002902,\n","         0.00995944, 0.01004517, 0.01001099, 0.00998356, 0.01000309,\n","         0.00996128, 0.00999849, 0.00989724, 0.00995473, 0.0099962 ]],\n","       dtype=float32)>,\n"," <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n"," array([[-3.96233751e-04,  1.57052223e-02,  1.41745778e-02,\n","         -3.32141481e-03,  7.49716815e-03, -1.27296066e-02,\n","         -1.10829398e-02,  8.34507681e-03,  8.95629171e-03,\n","          3.98326851e-03, -2.63246096e-04, -7.41923600e-03,\n","         -9.55925716e-05, -2.88352137e-03, -1.28554851e-02,\n","          6.89360593e-03, -2.15390860e-03, -4.14279476e-03,\n","         -2.12430512e-03,  4.95385146e-03, -1.31154237e-02,\n","          3.72438761e-03, -8.65468115e-04, -1.48835145e-02,\n","         -1.04142521e-02, -8.01153760e-03, -6.05320511e-03,\n","          1.20323393e-02, -3.92173463e-03, -6.69879001e-03,\n","         -9.37179755e-03, -7.43487291e-03]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n"," array([[[0.10034183],\n","         [0.09764397],\n","         [0.09787027],\n","         [0.09718679],\n","         [0.09810447],\n","         [0.09871785],\n","         [0.10040092],\n","         [0.10241324],\n","         [0.10377064],\n","         [0.10355002]]], dtype=float32)>)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n","loss_function = tf.keras.losses.SparseCategoricalCrossentropy()"],"metadata":{"id":"9XNOVsFK4ejq","executionInfo":{"status":"ok","timestamp":1736357939894,"user_tz":-60,"elapsed":541,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["import os\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"metadata":{"id":"FlduqxuH_4cy","executionInfo":{"status":"ok","timestamp":1736357963461,"user_tz":-60,"elapsed":240,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def train_step(inp, targ):#, enc_initial_state):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape: # we use the gradient tape to track all\n","  # the different operations happening in the network in order to be able\n","  # to compute the gradients later\n","\n","    enc_output, enc_state = encoder(inp)#,enc_initial_state) # the input sequence is fed to the\n","    # encoder to produce the encoder output and the encoder state\n","\n","    dec_state = enc_state # the initial state used in the decoder is the encoder\n","    # state\n","\n","    dec_input = tf.expand_dims(targ[:,0], axis=1) # the first decoder input\n","    # is the first sequence element of the target batch, which in our case\n","    # represents the <start> token for each sequence in the batch. This is\n","    # what we call the teacher forcing!\n","\n","    # Everything is set up for the first step, now we need to loop over the\n","    # teacher forcing sequence to produce the predictions, we already have\n","    # defined the first step (element 0) so we will loop from 1 to targ.shape[1]\n","    # which is the target sequence length\n","    for t in range(1, targ.shape[1]):\n","      # passing dec_input, dec_state and enc_output to the decoder\n","      # in order to produce the prediction, the new state, and the attention\n","      # weights which we will not need explicitely here\n","      pred, dec_state, _ = decoder(dec_input, enc_output, dec_state)\n","\n","      loss += loss_function(targ[:, t], pred) # we compare the prediction\n","      # produced by teacher forcing with the next element of the target and\n","      # increment the loss\n","\n","      # The new decoder input becomes the next element of the target sequence\n","      # which we just attempted to predict (teacher forcing)\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1])) # we divide the loss by the target\n","  # sequence's length to get the average loss across the sequence\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables # here\n","  # we concatenate the lists of trainable variables for the encoder and the\n","  # decoder\n","\n","  gradients = tape.gradient(loss, variables) # compute the gradient based on the\n","  # loss and the trainable variables\n","\n","  optimizer.apply_gradients(zip(gradients, variables)) # then update the model's\n","  # parameters\n","\n","  return batch_loss"],"metadata":{"id":"mlR8RzNv_-Kc","executionInfo":{"status":"ok","timestamp":1736357988727,"user_tz":-60,"elapsed":522,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["import time\n","EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(train_batch):\n","    batch_loss = train_step(inp, targ)\n","    total_loss += batch_loss\n","\n","    if batch % 10 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","\n","  # saving (checkpoint) the model every epoch\n","  checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss))\n","  print('Time taken for 1 epoch {} sec'.format(time.time() - start))\n","\n","  enc_input = X_val\n","  #classic encoder input\n","\n","  dec_input = tf.zeros(shape=(len(X_val),1))\n","  # the first decoder input is the special token 0\n","\n","  enc_out, enc_state = encoder(enc_input)#, initial_state)\n","  # we compute once and for all the encoder output and the encoder\n","  # h state and c state\n","\n","  dec_state = enc_state\n","  # The encoder h state and c state will serve as initial states for the\n","  # decoder\n","\n","  pred = []  # we'll store the predictions in here\n","\n","  # we loop over the expected length of the target, but actually the loop can run\n","  # for as many steps as we wish, which is the advantage of the encoder decoder\n","  # architecture\n","  for i in range(y_val.shape[1]-1):\n","    dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n","    # the decoder state is updated and we get the first prediction probability\n","    # vector\n","    decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n","    # we decode the softmax vector into and index\n","    pred.append(tf.expand_dims(dec_out,axis=1)) # update the prediction list\n","    dec_input = decoded_out # the previous pred will be used as the new input\n","\n","  pred = tf.concat(pred, axis=1).numpy()\n","  print(\"\\n val loss :\", loss_function(y_val[:,1:],pred),\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgeXEozkAEX7","executionInfo":{"status":"ok","timestamp":1736358404040,"user_tz":-60,"elapsed":383955,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"a7435db4-7ca8-412d-ea78-d71eec59ce41"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 3.8371\n","Epoch 1 Batch 10 Loss 3.8361\n","Epoch 1 Batch 20 Loss 3.8349\n","Epoch 1 Batch 30 Loss 3.8316\n","Epoch 1 Batch 40 Loss 3.8294\n","Epoch 1 Batch 50 Loss 3.8263\n","Epoch 1 Batch 60 Loss 3.8098\n","Epoch 1 Batch 70 Loss 3.7962\n","Epoch 1 Loss 302.0292\n","Time taken for 1 epoch 37.84857940673828 sec\n","\n"," val loss : tf.Tensor(4.525864, shape=(), dtype=float32) \n","\n","Epoch 2 Batch 0 Loss 3.7691\n","Epoch 2 Batch 10 Loss 3.7221\n","Epoch 2 Batch 20 Loss 3.6663\n","Epoch 2 Batch 30 Loss 3.6487\n","Epoch 2 Batch 40 Loss 3.5977\n","Epoch 2 Batch 50 Loss 3.5502\n","Epoch 2 Batch 60 Loss 3.5425\n","Epoch 2 Batch 70 Loss 3.4986\n","Epoch 2 Loss 284.9395\n","Time taken for 1 epoch 40.97912311553955 sec\n","\n"," val loss : tf.Tensor(4.1887555, shape=(), dtype=float32) \n","\n","Epoch 3 Batch 0 Loss 3.5079\n","Epoch 3 Batch 10 Loss 3.4317\n","Epoch 3 Batch 20 Loss 3.4282\n","Epoch 3 Batch 30 Loss 3.3740\n","Epoch 3 Batch 40 Loss 3.2935\n","Epoch 3 Batch 50 Loss 3.2137\n","Epoch 3 Batch 60 Loss 3.1122\n","Epoch 3 Batch 70 Loss 3.0325\n","Epoch 3 Loss 257.0478\n","Time taken for 1 epoch 36.00342130661011 sec\n","\n"," val loss : tf.Tensor(3.5136619, shape=(), dtype=float32) \n","\n","Epoch 4 Batch 0 Loss 2.8623\n","Epoch 4 Batch 10 Loss 2.6769\n","Epoch 4 Batch 20 Loss 2.5105\n","Epoch 4 Batch 30 Loss 2.3276\n","Epoch 4 Batch 40 Loss 2.1738\n","Epoch 4 Batch 50 Loss 2.0016\n","Epoch 4 Batch 60 Loss 1.8427\n","Epoch 4 Batch 70 Loss 1.6811\n","Epoch 4 Loss 173.6985\n","Time taken for 1 epoch 33.99033260345459 sec\n","\n"," val loss : tf.Tensor(1.8781163, shape=(), dtype=float32) \n","\n","Epoch 5 Batch 0 Loss 1.5436\n","Epoch 5 Batch 10 Loss 1.4024\n","Epoch 5 Batch 20 Loss 1.3226\n","Epoch 5 Batch 30 Loss 1.2177\n","Epoch 5 Batch 40 Loss 1.1284\n","Epoch 5 Batch 50 Loss 1.0638\n","Epoch 5 Batch 60 Loss 0.9758\n","Epoch 5 Batch 70 Loss 0.9329\n","Epoch 5 Loss 92.6325\n","Time taken for 1 epoch 34.75892496109009 sec\n","\n"," val loss : tf.Tensor(1.0340862, shape=(), dtype=float32) \n","\n","Epoch 6 Batch 0 Loss 0.8691\n","Epoch 6 Batch 10 Loss 0.7947\n","Epoch 6 Batch 20 Loss 0.7360\n","Epoch 6 Batch 30 Loss 0.7016\n","Epoch 6 Batch 40 Loss 0.6405\n","Epoch 6 Batch 50 Loss 0.5915\n","Epoch 6 Batch 60 Loss 0.5617\n","Epoch 6 Batch 70 Loss 0.5789\n","Epoch 6 Loss 52.3549\n","Time taken for 1 epoch 40.977484703063965 sec\n","\n"," val loss : tf.Tensor(0.6159737, shape=(), dtype=float32) \n","\n","Epoch 7 Batch 0 Loss 0.5059\n","Epoch 7 Batch 10 Loss 0.5119\n","Epoch 7 Batch 20 Loss 0.4520\n","Epoch 7 Batch 30 Loss 0.4158\n","Epoch 7 Batch 40 Loss 0.4246\n","Epoch 7 Batch 50 Loss 0.3692\n","Epoch 7 Batch 60 Loss 0.3884\n","Epoch 7 Batch 70 Loss 0.3356\n","Epoch 7 Loss 32.7502\n","Time taken for 1 epoch 35.025203704833984 sec\n","\n"," val loss : tf.Tensor(0.39513326, shape=(), dtype=float32) \n","\n","Epoch 8 Batch 0 Loss 0.3156\n","Epoch 8 Batch 10 Loss 0.3166\n","Epoch 8 Batch 20 Loss 0.3010\n","Epoch 8 Batch 30 Loss 0.2721\n","Epoch 8 Batch 40 Loss 0.2655\n","Epoch 8 Batch 50 Loss 0.2613\n","Epoch 8 Batch 60 Loss 0.2484\n","Epoch 8 Batch 70 Loss 0.2279\n","Epoch 8 Loss 22.0047\n","Time taken for 1 epoch 40.98998403549194 sec\n","\n"," val loss : tf.Tensor(0.27477238, shape=(), dtype=float32) \n","\n","Epoch 9 Batch 0 Loss 0.2141\n","Epoch 9 Batch 10 Loss 0.2205\n","Epoch 9 Batch 20 Loss 0.2026\n","Epoch 9 Batch 30 Loss 0.1940\n","Epoch 9 Batch 40 Loss 0.1910\n","Epoch 9 Batch 50 Loss 0.1782\n","Epoch 9 Batch 60 Loss 0.1708\n","Epoch 9 Batch 70 Loss 0.1534\n","Epoch 9 Loss 15.4106\n","Time taken for 1 epoch 35.8307409286499 sec\n","\n"," val loss : tf.Tensor(0.19320558, shape=(), dtype=float32) \n","\n","Epoch 10 Batch 0 Loss 0.1536\n","Epoch 10 Batch 10 Loss 0.1490\n","Epoch 10 Batch 20 Loss 0.1565\n","Epoch 10 Batch 30 Loss 0.1678\n","Epoch 10 Batch 40 Loss 0.1443\n","Epoch 10 Batch 50 Loss 0.1340\n","Epoch 10 Batch 60 Loss 0.1340\n","Epoch 10 Batch 70 Loss 0.1671\n","Epoch 10 Loss 12.5625\n","Time taken for 1 epoch 40.99768662452698 sec\n","\n"," val loss : tf.Tensor(0.15236668, shape=(), dtype=float32) \n","\n"]}]},{"cell_type":"code","source":["enc_input = X_val\n","#classic encoder input\n","\n","dec_input = tf.zeros(shape=(len(X_val),1))\n","# the first decoder input is the special token 0\n","\n","#initial_state = encoder.state_initializer(len(X_val))\n","\n","enc_out, enc_state = encoder(enc_input)#, initial_state)\n","# we compute once and for all the encoder output and the encoder\n","# h state and c state\n","\n","dec_state = enc_state\n","# The encoder h state and c state will serve as initial states for the\n","# decoder\n","\n","pred = []  # we'll store the predictions in here\n","\n","# we loop over the expected length of the target, but actually the loop can run\n","# for as many steps as we wish, which is the advantage of the encoder decoder\n","# architecture\n","for i in range(y_val.shape[1]-1):\n","  dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n","  # the decoder state is updated and we get the first prediction probability\n","  # vector\n","  decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n","  # we decode the softmax vector into and index\n","  pred.append(decoded_out) # update the prediction list\n","  dec_input = decoded_out # the previous pred will be used as the new input\n","\n","pred = tf.concat(pred, axis=-1).numpy()\n","for i in range(10):\n","  print(\"pred:\", pred[i,:].tolist())\n","  print(\"true:\", y_val[i,:].tolist()[1:])\n","  print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7c8X2pgAMKL","executionInfo":{"status":"ok","timestamp":1736358452334,"user_tz":-60,"elapsed":1246,"user":{"displayName":"marie-sophie chenevier","userId":"01317793668247273219"}},"outputId":"b0a1ed37-3b6a-450b-abd8-ce3baa32131b"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["pred: [33, 86, 45, 34, 56]\n","true: [33, 86, 45, 34, 56]\n","\n","\n","pred: [42, 97, 66, 28, 29]\n","true: [42, 97, 66, 28, 29]\n","\n","\n","pred: [54, 55, 61, 83, 71]\n","true: [54, 55, 61, 83, 71]\n","\n","\n","pred: [96, 35, 18, 82, 46]\n","true: [96, 35, 18, 82, 46]\n","\n","\n","pred: [28, 87, 24, 67, 77]\n","true: [28, 87, 24, 67, 77]\n","\n","\n","pred: [28, 73, 56, 36, 62]\n","true: [28, 73, 56, 36, 62]\n","\n","\n","pred: [85, 77, 10, 65, 39]\n","true: [85, 77, 10, 65, 39]\n","\n","\n","pred: [27, 19, 52, 68, 96]\n","true: [27, 19, 52, 68, 96]\n","\n","\n","pred: [36, 79, 54, 9, 43]\n","true: [36, 79, 54, 9, 43]\n","\n","\n","pred: [4, 72, 19, 30, 98]\n","true: [4, 72, 19, 30, 98]\n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PH9EGm6qB1Yl"},"execution_count":null,"outputs":[]}]}